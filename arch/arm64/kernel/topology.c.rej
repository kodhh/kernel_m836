--- arch/arm64/kernel/topology.c
+++ arch/arm64/kernel/topology.c
@@ -196,11 +198,7 @@ struct cpu_efficiency {
  * Table of relative efficiency of each processors
  * The efficiency value must fit in 20bit and the final
  * cpu_scale value must be in the range
- *   0 < cpu_scale < 3*SCHED_POWER_SCALE/2
- * in order to return at most 1 when DIV_ROUND_CLOSEST
- * is used to compute the capacity of a CPU.
- * Processors that are not defined in the table,
- * use the default SCHED_POWER_SCALE value for cpu_scale.
+ *   0 < cpu_scale < SCHED_CAPACITY_SCALE
  */
 static const struct cpu_efficiency table_efficiency[] = {
 	{ "arm,cortex-a72", 4186 },
@@ -213,12 +211,14 @@ static unsigned long *__cpu_capacity;
 
 static unsigned long middle_capacity = 1;
 
-static DEFINE_PER_CPU(unsigned long, cpu_efficiency) = SCHED_POWER_SCALE;
+#ifdef TJK_HMP
+static DEFINE_PER_CPU(unsigned long, cpu_efficiency) = SCHED_CAPACITY_SCALE;
 
 unsigned long arch_get_cpu_efficiency(int cpu)
 {
 	return per_cpu(cpu_efficiency, cpu);
 }
+#endif
 
 /*
  * Iterate all CPUs' descriptor in DT and compute the efficiency
@@ -361,17 +381,19 @@ static void __init parse_dt_cpu_power(void)
 
 	/* compute a middle_capacity factor that will ensure that the capacity
 	 * of an 'average' CPU of the system will be as close as possible to
-	 * SCHED_POWER_SCALE, which is the default value, but with the
+	 * SCHED_CAPACITY_SCALE, which is the default value, but with the
 	 * constraint explained near table_efficiency[].
 	 */
 	if (4 * max_capacity < (3 * (max_capacity + min_capacity)))
               middle_capacity = (min_capacity + max_capacity)
-				>> (SCHED_POWER_SHIFT+1);
+				>> (SCHED_CAPACITY_SHIFT+1);
 	else
 		middle_capacity = ((max_capacity / 3)
-				>> (SCHED_POWER_SHIFT-1)) + 1;
+				>> (SCHED_CAPACITY_SHIFT-1)) + 1;
 }
 
+#ifdef TJK_HMP
+
 /*
  * Look for a customed capacity of a CPU in the cpu_topo_data table during the
  * boot. The update of all CPUs is in O(n^2) for heteregeneous system but the
@@ -511,14 +562,40 @@ static void __init reset_cpu_topology(void)
 	}
 }
 
-static void __init reset_cpu_power(void)
+static void __init reset_cpu_capacity(void)
 {
 	unsigned int cpu;
 
 	for_each_possible_cpu(cpu)
-		set_power_scale(cpu, SCHED_POWER_SCALE);
+		set_capacity_scale(cpu, SCHED_CAPACITY_SCALE);
 }
 
+static inline const struct sched_group_energy *cpu_cluster_energy(int cpu)
+{
+ 	struct sched_group_energy *sge = sge_array[cpu][SD_LEVEL1];
+
+ 	if (!sge) {
+ 		pr_warn("Invalid sched_group_energy for Cluster%d\n", cpu);
+ 		return NULL;
+ 	}
+
+ 	return sge;
+}
+
+static inline int cpu_corepower_flags(void)
+{
+ 	return SD_SHARE_PKG_RESOURCES  | SD_SHARE_POWERDOMAIN | \
+ 	       SD_SHARE_CAP_STATES;
+}
+
+static struct sched_domain_topology_level arm64_topology[] = {
+#ifdef CONFIG_SCHED_MC
+ 	{ cpu_coregroup_mask, cpu_corepower_flags, cpu_core_energy, SD_INIT_NAME(MC) },
+#endif
+ 	{ cpu_cpu_mask, 0, cpu_cluster_energy, SD_INIT_NAME(DIE) },
+ 	{ NULL, },
+};
+
 void __init init_cpu_topology(void)
 {
 	reset_cpu_topology();
@@ -529,7 +606,10 @@ void __init init_cpu_topology(void)
 	 */
 	if (parse_dt_topology())
 		reset_cpu_topology();
+        else
+ 		set_sched_topology(arm64_topology);
 
-	reset_cpu_power();
-	parse_dt_cpu_power();
+	reset_cpu_capacity();
+ 	parse_dt_cpu_capacity();
+ 	init_sched_energy_costs();
 }
